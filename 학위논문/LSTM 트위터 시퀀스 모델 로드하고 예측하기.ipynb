{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 로드\n",
    "lstm 모델 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "tweets = pd.read_csv(\"./data/sentiment140_training.1600000.processed.noemoticon.csv\", encoding = 'latin-1', header = None, names = cols)\n",
    "tweets.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = '앱티브_2019kia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#일반 데이터용.\n",
    "file_cols = ['date', 'time', 'user', 'text']\n",
    "file = pd.read_csv(\"./전처리/preprocessed/전처리최종_{}.csv\".format(brand), encoding = 'utf-8', header = None, names = file_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>key</th>\n",
       "      <th>문서 번호</th>\n",
       "      <th>가장 비중이 높은 토픽</th>\n",
       "      <th>가장 높은 토픽의 비중</th>\n",
       "      <th>각 토픽의 비중</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-08</td>\n",
       "      <td>23:37:59</td>\n",
       "      <td>Olafur_Tomasson</td>\n",
       "      <td>angry wish not buy kia last year !</td>\n",
       "      <td>2018-02-0823:37:59Olafur_Tomasson</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5969</td>\n",
       "      <td>[(0, 0.017873356), (1, 0.01786835), (2, 0.2957...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-02-08</td>\n",
       "      <td>23:37:31</td>\n",
       "      <td>HoopVideos_NET</td>\n",
       "      <td>kia rookie ladder josh jackson enters mix ben ...</td>\n",
       "      <td>2018-02-0823:37:31HoopVideos_NET</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.6288</td>\n",
       "      <td>[(2, 0.08842112), (5, 0.62881666), (7, 0.23793...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-02-08</td>\n",
       "      <td>23:37:03</td>\n",
       "      <td>HillaryHubOLA</td>\n",
       "      <td>kia os name lili fox just find book class read...</td>\n",
       "      <td>2018-02-0823:37:03HillaryHubOLA</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3863</td>\n",
       "      <td>[(0, 0.08687749), (1, 0.10005543), (3, 0.38631...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-02-08</td>\n",
       "      <td>23:36:51</td>\n",
       "      <td>HoopVideos_NET</td>\n",
       "      <td>kia rookie ladder josh jackson enters mix ben ...</td>\n",
       "      <td>2018-02-0823:36:51HoopVideos_NET</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.6288</td>\n",
       "      <td>[(2, 0.08843764), (5, 0.6287973), (7, 0.237939...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-02-08</td>\n",
       "      <td>23:36:36</td>\n",
       "      <td>RamknickL</td>\n",
       "      <td>just ! recently add kia sorento inventory . check</td>\n",
       "      <td>2018-02-0823:36:36RamknickL</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.6387</td>\n",
       "      <td>[(0, 0.020846326), (1, 0.020849146), (2, 0.020...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1        date      time  \\\n",
       "0           1             1               1  2018-02-08  23:37:59   \n",
       "1           2             2               2  2018-02-08  23:37:31   \n",
       "2           3             3               3  2018-02-08  23:37:03   \n",
       "3           4             4               4  2018-02-08  23:36:51   \n",
       "4           5             5               5  2018-02-08  23:36:36   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  Olafur_Tomasson                 angry wish not buy kia last year !   \n",
       "1   HoopVideos_NET  kia rookie ladder josh jackson enters mix ben ...   \n",
       "2    HillaryHubOLA  kia os name lili fox just find book class read...   \n",
       "3   HoopVideos_NET  kia rookie ladder josh jackson enters mix ben ...   \n",
       "4        RamknickL  just ! recently add kia sorento inventory . check   \n",
       "\n",
       "                                 key  문서 번호  가장 비중이 높은 토픽  가장 높은 토픽의 비중  \\\n",
       "0  2018-02-0823:37:59Olafur_Tomasson      1           7.0        0.5969   \n",
       "1   2018-02-0823:37:31HoopVideos_NET      2           5.0        0.6288   \n",
       "2    2018-02-0823:37:03HillaryHubOLA      3           3.0        0.3863   \n",
       "3   2018-02-0823:36:51HoopVideos_NET      4           5.0        0.6288   \n",
       "4        2018-02-0823:36:36RamknickL      5           5.0        0.6387   \n",
       "\n",
       "                                            각 토픽의 비중  \n",
       "0  [(0, 0.017873356), (1, 0.01786835), (2, 0.2957...  \n",
       "1  [(2, 0.08842112), (5, 0.62881666), (7, 0.23793...  \n",
       "2  [(0, 0.08687749), (1, 0.10005543), (3, 0.38631...  \n",
       "3  [(2, 0.08843764), (5, 0.6287973), (7, 0.237939...  \n",
       "4  [(0, 0.020846326), (1, 0.020849146), (2, 0.020...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#전체 합본데이터용\n",
    "file = pd.read_csv(\"./합본데이터결과/기아합본_외국어문헌제외토픽테이블.csv\", encoding = 'utf-8', header = 0)\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94141"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "sentences = list(tweets['text'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))\n",
    "y = tweets['sentiment']\n",
    "\n",
    "#sentiment를 긍정은 1, 부정은 0으로 수정\n",
    "y = np.array(list(map(lambda x: 1 if x == 4 else 0, y)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "#Preparing the Embedding Layer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 69\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ohi02\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 69, 100)           50785600  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 50,902,977\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 50,785,600\n",
      "_________________________________________________________________\n",
      "320000/320000 [==============================] - 267s 835us/sample - loss: 0.4027 - acc: 0.8178\n",
      "복원된 모델의 정확도: 81.78%\n"
     ]
    }
   ],
   "source": [
    "new_model = keras.models.load_model('./lstm모델/DB2048_twitter100D_69_30_lstm_model.h5', compile=False)\n",
    "new_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "new_model.summary()\n",
    "loss, acc = new_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"복원된 모델의 정확도: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 로드하고 여기서부터만 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[[0.1980812 ]\n",
      " [0.94263864]\n",
      " [0.04509246]\n",
      " ...\n",
      " [0.93017447]\n",
      " [0.39690453]\n",
      " [0.44428813]]\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "instance = []\n",
    "sentences = list(file['text'])\n",
    "for sen in sentences:\n",
    "    sen = str(sen)\n",
    "    instance.append(preprocess_text(sen))\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(instance)\n",
    "instance = tokenizer.texts_to_sequences(instance)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 69\n",
    "\n",
    "instance= pad_sequences(instance, padding='post', maxlen=maxlen)\n",
    "results = new_model.predict(instance)\n",
    "classes = new_model.predict_classes(instance)\n",
    "print(type(results))\n",
    "print(type(classes))\n",
    "\n",
    "print(results)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = '기아합본_외국어문헌제외토픽테이블'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./감성결과/9일간감성결과/감성결과_{}.csv'.format(brand), 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(file)):\n",
    "        f.write('{},{},{},{},{},{},{}\\n'.format(i, file['date'][i], file['time'][i], file['user'][i], file['text'][i], results[i], classes[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#합본데이터용\n",
    "with open('./감성결과/9일간감성결과/감성결과2_{}.csv'.format(brand), 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(file)):\n",
    "        f.write('{},{},{},{},{},{},{},{},{},{},{},{}\\n'.format(i, file['date'][i], file['time'][i], file['user'][i], file['text'][i], results[i], classes[i], file['key'][i],file['문서 번호'][i],file['가장 비중이 높은 토픽'][i],file['가장 높은 토픽의 비중'][i],file['각 토픽의 비중'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#합본데이터용\n",
    "with open('./감성결과/9일간감성결과/감성결과3_{}.csv'.format(brand), 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(file)):\n",
    "        f.write('{},{},{},{},{},{},{},{},{},{},{}\\n'.format(i, file['date'][i], file['time'][i], file['user'][i], file['text'][i], results[i], classes[i], file['key'][i],file['문서 번호'][i],file['가장 비중이 높은 토픽'][i],file['가장 높은 토픽의 비중'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "위의꺼 cols = ['i','date', 'time', 'user', 'text', 'results', 'classes', 'key','문서 번호','가장 비중이 높은 토픽','가장 높은 토픽의 비중','각 토픽의 비중']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
