{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import pyTextMiner as ptm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocessing_english():\n",
    "    tweet_raw = pd.read_csv('./SuperbowlData/Filtered_2019hyundai_190131_190207.csv')\n",
    "\n",
    "    corpus = []\n",
    "    for i in range(len(tweet_raw)):\n",
    "        corpus.append(\"{}\".format(tweet_raw['text'][i]))#.split(\".\"))\n",
    "        for doc in corpus:\n",
    "            doc = doc.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"%\", \"\").replace(\"? 셳\", \"'t\").replace(\"?셳\", \"'t\").replace(\"? 셲\", \"'s\").replace(\"?셲\", \"'s\")\\\n",
    "                .replace(\"? 쁥\", \"'h\").replace(\"?쁥\", \"'h\").replace(\"? 쁲\", \"'s\").replace(\"?쁲\", \"'s\").replace(\"? 셱\", \"'r\").replace(\"?셱\", \"'r\")\\\n",
    "                .replace(\"? 셫\", \"'m\").replace(\"?셫\", \"'m\").replace(\"? 쁶\", \"'w\").replace(\"?쁶\", \"'w\")\n",
    "            \n",
    "    pipeline = ptm.Pipeline(ptm.splitter.NLTK(), ptm.tokenizer.Word(),\n",
    "                           ptm.helper.StopwordFilter('./data/english_stopwords.txt'),\n",
    "                           ptm.tagger.NLTK(), ptm.lemmatizer.WordNet(), #kp.helper.POSFilter('N*', 'J*'),\n",
    "                           ptm.helper.SelectWordOnly())#, kp.ngram.NGramTokenizer())\n",
    "   \n",
    "    result = pipeline.processCorpus(corpus)\n",
    "    \n",
    "    f_output = open('./SuperbowlData/전처리실험_2019현대.csv', 'w', encoding='utf-8', newline='')\n",
    "    csv_writer = csv.writer(f_output)\n",
    "    csv_writer.writerow(['date', 'user_name', 'text'])\n",
    "\n",
    "    for i, doc in enumerate(result):\n",
    "        sent = list(map(\" \".join, doc))\n",
    "        csv_writer.writerow([tweet_raw['date'][i], tweet_raw['user_name'][i], \". \".join(sent)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ohi02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ohi02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tweet_raw = pd.read_csv('./SuperbowlData/Filtered_2019hyundai_190131_190207.csv')\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(tweet_raw)):\n",
    "    corpus.append(\"{}\".format(tweet_raw['text'][i]))#.split(\".\"))\n",
    "    for doc in corpus:\n",
    "        doc = doc.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"%\", \"\").replace(\"? 셳\", \"'t\").replace(\"?셳\", \"'t\").replace(\"? 셲\", \"'s\").replace(\"?셲\", \"'s\")\\\n",
    "            .replace(\"? 쁥\", \"'h\").replace(\"?쁥\", \"'h\").replace(\"? 쁲\", \"'s\").replace(\"?쁲\", \"'s\").replace(\"? 셱\", \"'r\").replace(\"?셱\", \"'r\")\\\n",
    "            .replace(\"? 셫\", \"'m\").replace(\"?셫\", \"'m\").replace(\"? 쁶\", \"'w\").replace(\"?쁶\", \"'w\")\n",
    "            \n",
    "pipeline = ptm.Pipeline(ptm.splitter.NLTK(), ptm.tokenizer.Word(),\n",
    "                        ptm.helper.StopwordFilter('./data/english_stopwords.txt'),\n",
    "                        ptm.tagger.NLTK(), ptm.lemmatizer.WordNet(), #kp.helper.POSFilter('N*', 'J*'),\n",
    "                        ptm.helper.SelectWordOnly())#, kp.ngram.NGramTokenizer())\n",
    "   \n",
    "result = pipeline.processCorpus(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Yeah', 'they', 'make', 'them', 'in', 'very', 'cheesy', 'way', 'be', 'squirm', 'while', 'see', 'it', 'and', 'now', 'want', 'to', 'be', 'take', 'on', 'date', 'in', 'Hyundai', 'car', 'with', 'man', 'the', 'pull', 'the', 'moon', 'closer', 'for', 'me']], [['The', 'price', 'have', 'change', 'on', 'our', '2017', 'Hyundai', 'Santa', 'Fe'], ['Take', 'look', ':', 'http', ':', '//www.carsforsale.com/vehicle/details/3-25780950']], [['The', 'price', 'for', '2015', 'Hyundai', 'Genesis', 'be', '$', '2,299', 'now'], ['Take', 'look', ':', 'http', ':', '//www.carsforsale.com/vehicle/details/3-26627002']], [['The', 'price', 'for', '2017', 'Hyundai', 'Santa', 'Fe', 'be', '$', '2,499', 'now'], ['Take', 'look', ':', 'http', ':', '//www.carsforsale.com/vehicle/details/3-25780910']], [['The', 'price', 'for', '2007', 'Hyundai', 'Elantra', 'be', '$', '4,582', 'now'], ['Take', 'look', ':', 'http', ':', '//www.carsforsale.com/vehicle/details/3-14840194']], [['The', 'price', 'have', 'change', 'on', 'our', '2015', 'Hyundai', 'Sonata', 'Hybrid'], ['Take', 'look', ':', 'http', ':', '//www.carsforsale.com/vehicle/details/3-25089180']], [['The', 'price', 'for', '2006', 'Hyundai', 'Sonata', 'be', '$', '4,682', 'now'], ['Take', 'look', ':', 'http', ':', '//www.carsforsale.com/vehicle/details/3-7423589']], [['Just', 'in', '!'], ['We', 'have', 'recently', 'add', '2012', 'Hyundai', 'Sonata', 'to', 'our', 'inventory'], ['Check', 'it', 'out', ':', 'http', ':', '//tinyurl.com/y759rft5']], [['COVERCRAFT', 'SS2510PCTN', 'Ss', 'Frt', 'fit', 'Hyundai', 'Tucson', 'http', ':', '//hecate.feuercloud.info/US/categories/twt/', '?', 'item=303054613196']], [['The', 'price', 'have', 'change', 'on', 'our', '2013', 'Hyundai', 'Sonata'], ['Take', 'look', ':', 'http', ':', '//www.carsforsale.com/vehicle/details/3-26840455']]]\n"
     ]
    }
   ],
   "source": [
    "print(result[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "f_output = open('./SuperbowlData/전처리실험_2019현대.csv', 'w', encoding='utf-8', newline='')\n",
    "csv_writer = csv.writer(f_output)\n",
    "csv_writer.writerow(['date', 'user_name', 'text'])\n",
    "\n",
    "for i, doc in enumerate(result):\n",
    "    sent = list(map(\" \".join, doc))\n",
    "    csv_writer.writerow([tweet_raw['date'][i], tweet_raw['user_name'][i], \". \".join(sent)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ohi02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ohi02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "preprocessing_english()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 감성분석하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyTextMiner as ptm\n",
    "import io\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import nltk\n",
    "\n",
    "class EnglishDictionarySentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        name = 'EnglishDictionarySentimentAnalyzer'\n",
    "\n",
    "    def createDictionary(self):\n",
    "        nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\ohi02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-05184e443793>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mEnglishDictionarySentimentAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mgrand_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "EnglishDictionarySentimentAnalyzer().createDictionary()\n",
    "\n",
    "for doc in result:\n",
    "    grand_score = 0.0\n",
    "    count = 0\n",
    "    for sent in doc:\n",
    "        for _str in sent:\n",
    "            _str[0]\n",
    "            _str[1]\n",
    "            pos = ''\n",
    "            if (str(_str[1]).startswith(\"N\")):\n",
    "                pos = 'n'\n",
    "            elif (str(_str[1]).startswith(\"J\")):\n",
    "                pos = 'a'\n",
    "            elif (str(_str[1]).startswith(\"V\")):\n",
    "                pos = 'v'\n",
    "            try:\n",
    "                if (len(pos) > 0):\n",
    "                    score = 0.0\n",
    "                    breakdown = swn.senti_synset(str(_str[0]) + '.'+ pos + '.01')\n",
    "                    #print(str(breakdown) + \" \" + str(breakdown.pos_score()) + \" \" + str(breakdown.neg_score()) + \" \" + str(breakdown.obj_score()))\n",
    "                    if (breakdown.pos_score() > breakdown.neg_score()):\n",
    "                        score = breakdown.pos_score()\n",
    "                        count += 1\n",
    "                    elif (breakdown.pos_score() < breakdown.neg_score() & breakdown.neg_score() != 0.0):\n",
    "                        score = -breakdown.neg_score()\n",
    "                        count += 1\n",
    "                    grand_score += score\n",
    "            except:\n",
    "                pos = ''\n",
    "\n",
    "    if count ==0:\n",
    "        pass\n",
    "    else:\n",
    "        avg_score = grand_score/count\n",
    "        print(\"Average Sentiment Score \" + str(avg_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
