{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling, LDA 구현\n",
    "\n",
    "이번 글에서는 말뭉치로부터 토픽을 추출하는 토픽모델링(Topic Modeling) 기법 가운데 하나인 잠재디리클레할당(Latent Dirichlet Allocation, LDA)을 파이썬 코드로 구현하는 법을 살펴보도록 하겠습니다. 이 글은 ‘밑바닥부터 시작하는 데이터 과학(조엘 그루스 지음, 인사이트 펴냄)’을 정리하였음을 먼저 밝힙니다. LDA 기법 자체에 대한 자세한 내용은 이곳을 참고하시면 좋을 것 같습니다. 그럼 시작하겠습니다.\n",
    "\n",
    "https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/06/01/LDA/\n",
    "https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/07/09/lda/\n",
    "\n",
    "수식\n",
    "LDA 모델을 모두 정리하면 d​번째 문서 i​번째 단어의 토픽 zd,i​가 j​번째에 할당될 확률은 다음과 같이 쓸 수 있습니다.\n",
    "\n",
    "p(zd,i=j|z−i,w)=nd,k+αj∑Ki=1(nd,i+αi)×vk,wd,n+βwd,n∑Vj=1vk,j+βj=AB\n",
    "위 수식의 표기를 정리한 표는 다음과 같습니다.\n",
    "nd,k\tk번째 토픽에 할당된 d번째 문서의 단어 빈도\n",
    "vk,wd,n\t전체 말뭉치에서 k번째 토픽에 할당된 단어 wd,n의 빈도\n",
    "wd,n\td번째 문서에 n번째로 등장한 단어\n",
    "α\t문서의 토픽 분포 생성을 위한 디리클레 분포 파라메터\n",
    "β\t토픽의 단어 분포 생성을 위한 디리클레 분포 파라메터\n",
    "K\t사용자가 지정하는 토픽 수\n",
    "V\t말뭉치에 등장하는 전체 단어 수\n",
    "A\td번째 문서가 k번째 토픽과 맺고 있는 연관성 정도\n",
    "B\td번째 문서의 n번째 단어(wd,n)가 k번째 토픽과 맺고 있는 연관성 정도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변수 선언\n",
    "LDA 학습을 위해서는 변수를 먼저 선언해주어야 합니다. 다음과 같습니다.\n",
    "(documents를 먼저 선언해줄 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e34b3f523f36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Counter로 구성된 리스트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 각 Counter는 각 문서를 의미\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdocument_topic_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# 각 단어가 각 토픽에 할당되는 횟수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 각 토픽이 각 문서에 할당되는 횟수\n",
    "# Counter로 구성된 리스트\n",
    "# 각 Counter는 각 문서를 의미\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "\n",
    "# 각 단어가 각 토픽에 할당되는 횟수\n",
    "# Counter로 구성된 리스트\n",
    "# 각 Counter는 각 토픽을 의미\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "# 각 토픽에 할당되는 총 단어수\n",
    "# 숫자로 구성된 리스트\n",
    "# 각각의 숫자는 각 토픽을 의미함\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "# 각 문서에 포함되는 총 단어수\n",
    "# 숫자로 구성된 리스트\n",
    "# 각각의 숫자는 각 문서를 의미함\n",
    "document_lengths = map(len, documents)\n",
    "\n",
    "# 단어 종류의 수\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "V = len(distinct_words)\n",
    "\n",
    "# 총 문서의 수\n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 notation\tcode 변수명\n",
    "nd,k\tdocument_topic_counts\n",
    "vk,wd,n\ttopic_word_counts\n",
    "ΣKi=1nd,i\tdocument_lengths\n",
    "ΣVj=1vk,j\ttopic_counts\n",
    "\n",
    "예컨대 세번째 문서 가운데 토픽 1과 관련있는 단어수는 다음과 같습니다.\n",
    "\n",
    "document_topic_counts[3][1]\n",
    "\n",
    "nlp라는 단어가 토픽2와 연관지어 등장한 횟수는 다음과 같습니다.\n",
    "\n",
    "topic_word_counts[2][‘nlp’]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 새로운 topic 계산하기\n",
    "d번째 문서 i번째 단어의 토픽 zd,i가 j번째에 할당될 확률은 A와 B를 곱해 구합니다. 아래 코드에서 p_topic_given_document가 A, p_word_given_topic이 B입니다. topic_weight 함수는 이 둘을 곱한 값이라는 걸 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    # 문서 d의 모든 단어 가운데 topic에 속하는\n",
    "    # 단어의 비율 (alpha를 더해 smoothing)\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    # topic에 속한 단어 가운데 word의 비율\n",
    "    # (beta를 더해 smoothing)\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    # 문서와 문서의 단어가 주어지면\n",
    "    # k번째 토픽의 weight를 반환\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AB 를 구했으니 이를 바탕으로 샘플링을 하여 zd,i에 새로운 topic을 할당할 수 있습니다. 그 코드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "import random\n",
    "def sample_from(weights):\n",
    "    # i를 weights[i] / sum(weights)\n",
    "    # 확률로 반환\n",
    "    total = sum(weights)\n",
    "    # 0과 total 사이를 균일하게 선택\n",
    "    rnd = total * random.random()\n",
    "    # 아래 식을 만족하는 가장 작은 i를 반환\n",
    "    # weights[0] + ... + weights[i] >= rnd\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference\n",
    "다음과 같은 데이터가 주어졌다고 칩시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [[\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 inference에 필요한 기초 데이터를 만듭니다. 토픽수 K 등 하이퍼파라메터를 정하고, 각 단어를 임의의 토픽에 배정한 뒤 필요한 숫자를 세어봐야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "# topic 수 지정\n",
    "K=4\n",
    "\n",
    "# 각 단어를 임의의 토픽에 랜덤 배정\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                    for document in documents]\n",
    "\n",
    "# 위와 같이 랜덤 초기화한 상태에서 \n",
    "# AB를 구하는 데 필요한 숫자를 세어봄\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 목표는 ‘토픽-단어’와 ‘문서-토픽’에 대한 결합확률분포(unknown)로부터 표본을 얻는 것이므로, 깁스샘플링을 수행하면 됩니다. iteration은 1000으로 설정했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "            # 깁스 샘플링 수행을 위해\n",
    "            # 샘플링 대상 word와 topic을 제외하고 세어봄\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # 깁스 샘플링 대상 word와 topic을 제외한 \n",
    "            # 말뭉치 모든 word의 topic 정보를 토대로\n",
    "            # 샘플링 대상 word의 새로운 topic을 선택\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # 샘플링 대상 word의 새로운 topic을 반영해 \n",
    "            # 말뭉치 정보 업데이트\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일럿 실험 결과\n",
    "inference 결과 첫번째 문서의 토픽 비중은 다음과 같습니다. 전체 7개 단어 가운데 0번째 토픽과 관련된 단어가 4개 1번째 토픽 단어가 3개입니다. 따라서 이 문서는 첫번째 토픽일 확률이 가장 높네요.\n",
    "\n",
    ">> document_topic_counts[0]\n",
    "\n",
    "Counter({0: 4, 2: 3, 1: 0, 3: 0})\n",
    "\n",
    "첫번째 토픽의 단어 비중은 다음과 같습니다. Java, Big Data, Hadoop, deep learning 등 단어의 빈도(비중)가 높네요. 이 토픽은 대략 ‘Big Data’에 해당하는 주제인 것 같다는 느낌이 듭니다.\n",
    "\n",
    ">> topic_word_counts[0]\n",
    "\n",
    "Counter({‘Java’: 3, ‘Big Data’: 3, ‘Hadoop’: 2, ‘deep learning’: 2, ‘artificial intelligence’: 2, ‘C++’: 2, ‘neural networks’: 1, ‘Storm’: 1, ‘programming languages’: 1, ‘MapReduce’: 1, ‘Haskell’: 1, ‘probability’: 0, ‘Mahout’: 0, ‘NoSQL’: 0, ‘MySQL’: 0, ‘regression’: 0, ‘statistics’: 0, ‘Postgres’: 0, ‘Python’: 0, ‘mathematics’: 0, ‘Spark’: 0, ‘numpy’: 0, ‘pandas’: 0, ‘theory’: 0, ‘libsvm’: 0, ‘scipy’: 0, ‘R’: 0, ‘HBase’: 0, ‘decision trees’: 0, ‘MongoDB’: 0, ‘scikit-learn’: 0, ‘machine learning’: 0, ‘databases’: 0, ‘statsmodels’: 0, ‘support vector machines’: 0, ‘Cassandra’: 0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
