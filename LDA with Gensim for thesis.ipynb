{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topic Modeling with Gensim\n",
    "https://wikidocs.net/24949 -> 전처리 및 LSA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = '2019kia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>23:59:56</td>\n",
       "      <td>IAmBigHead901</td>\n",
       "      <td>nigga just sign name not read nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>23:59:13</td>\n",
       "      <td>Kvydvnise</td>\n",
       "      <td>now kia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>23:57:27</td>\n",
       "      <td>Chicago_Wolves</td>\n",
       "      <td>sure ? heat wave come game saturday . weather ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>23:57:24</td>\n",
       "      <td>akemiutau_bot</td>\n",
       "      <td>kia really mature sound . like voice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>23:56:54</td>\n",
       "      <td>snikastyle</td>\n",
       "      <td>never understand used car lot open hood car . ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      time            user  \\\n",
       "0  2019-01-30  23:59:56   IAmBigHead901   \n",
       "1  2019-01-30  23:59:13       Kvydvnise   \n",
       "2  2019-01-30  23:57:27  Chicago_Wolves   \n",
       "3  2019-01-30  23:57:24   akemiutau_bot   \n",
       "4  2019-01-30  23:56:54      snikastyle   \n",
       "\n",
       "                                                text  \n",
       "0              nigga just sign name not read nothing  \n",
       "1                                            now kia  \n",
       "2  sure ? heat wave come game saturday . weather ...  \n",
       "3               kia really mature sound . like voice  \n",
       "4  never understand used car lot open hood car . ...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 논문용\n",
    "import pandas as pd\n",
    "file_cols = ['date', 'time', 'user', 'text']\n",
    "file = pd.read_csv(\"./전처리/preprocessed/전처리최종_{}.csv\".format(brand), encoding = 'utf-8', header = None, names = file_cols)\n",
    "#file = pd.read_csv(\"./전처리/preprocessed/전처리최종_앱티브_{}.csv\".format(brand), encoding = 'utf-8', header = None, names = file_cols)\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-09-23</td>\n",
       "      <td>23:59:32</td>\n",
       "      <td>Netta_World</td>\n",
       "      <td>love u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-09-23</td>\n",
       "      <td>23:59:02</td>\n",
       "      <td>Miller_Geek</td>\n",
       "      <td>honestly windows bring back linux . every o is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-09-23</td>\n",
       "      <td>23:58:57</td>\n",
       "      <td>4SideRell</td>\n",
       "      <td>isnt funny serious matter kia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-09-23</td>\n",
       "      <td>23:55:15</td>\n",
       "      <td>CowboyDBWest</td>\n",
       "      <td>nope . re one people tell truth garbage . saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-09-23</td>\n",
       "      <td>23:55:00</td>\n",
       "      <td>SiSi_Simply</td>\n",
       "      <td>lmbooo kia ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25495</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>0:01:16</td>\n",
       "      <td>zervos19</td>\n",
       "      <td>kia sportage lx fwd silver suv door view detai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25496</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>0:00:32</td>\n",
       "      <td>NewsView100</td>\n",
       "      <td>job come america great but nowhere near great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25497</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>0:00:17</td>\n",
       "      <td>Wayne75362249</td>\n",
       "      <td>impeachment proceeding important u . fail heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25498</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>0:00:16</td>\n",
       "      <td>Azula163</td>\n",
       "      <td>bush not pay medicare d obama take debt also ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25499</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>0:00:05</td>\n",
       "      <td>GigiGloriby</td>\n",
       "      <td>automate job ... bot ... need ! ! ! yanggang p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      time           user  \\\n",
       "0      2019-09-23  23:59:32    Netta_World   \n",
       "1      2019-09-23  23:59:02    Miller_Geek   \n",
       "2      2019-09-23  23:58:57      4SideRell   \n",
       "3      2019-09-23  23:55:15   CowboyDBWest   \n",
       "4      2019-09-23  23:55:00    SiSi_Simply   \n",
       "...           ...       ...            ...   \n",
       "25495  2019-10-01   0:01:16       zervos19   \n",
       "25496  2019-10-01   0:00:32    NewsView100   \n",
       "25497  2019-10-01   0:00:17  Wayne75362249   \n",
       "25498  2019-10-01   0:00:16       Azula163   \n",
       "25499  2019-10-01   0:00:05    GigiGloriby   \n",
       "\n",
       "                                                    text  \n",
       "0                                                 love u  \n",
       "1      honestly windows bring back linux . every o is...  \n",
       "2                          isnt funny serious matter kia  \n",
       "3      nope . re one people tell truth garbage . saw ...  \n",
       "4                                           lmbooo kia ?  \n",
       "...                                                  ...  \n",
       "25495  kia sportage lx fwd silver suv door view detai...  \n",
       "25496  job come america great but nowhere near great ...  \n",
       "25497  impeachment proceeding important u . fail heal...  \n",
       "25498  bush not pay medicare d obama take debt also ....  \n",
       "25499  automate job ... bot ... need ! ! ! yanggang p...  \n",
       "\n",
       "[25500 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#앱티브 : 10/1 까지만 다시 할당\n",
    "file = file.loc[file['date'] != '2019-10-02']\n",
    "file[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23912\n"
     ]
    }
   ],
   "source": [
    "#논문용\n",
    "documents = file['text'].tolist()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련에 사용할 뉴스 데이터의 개수는 11314여개임. 이 중 첫번째 훈련용 뉴스를 출력해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "nigga just sign name not read nothing\n"
     ]
    }
   ],
   "source": [
    "#논문용\n",
    "print(type(documents))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스 데이터에는 특수문자가 포함된 다수의 영어문장으로 구성되어져 있습니다. 이런 형식의 뉴스가 총 11,314개 존재합니다. 사이킷런이 제공하는 뉴스 데이터에서 target_name에는 본래 이 뉴스 데이터가 어떤 20개의 카테고리를 갖고있었는지가 저장되어져 있습니다. 이를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#논문용\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "total_df = pd.DataFrame({'document': documents})\n",
    "#특수문자 제거\n",
    "total_df['clean_doc']= total_df['document'].str.replace(\"[^a-zA-Z]\", \" \").fillna(\"\")\n",
    "# 길이가 2이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "total_df['clean_doc'] = total_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "# 전체 단어에 대한 소문자 변환(이미 전처리 단계에서 완료)\n",
    "#total_df['clean_doc'] = total_df['clean_doc'].apply(lambda x: x.lower())\n",
    "#total_df['clean_doc'] = total_df['clean_doc'].str.replace(\"'ve\", \" have\").str.replace(\"'s\", \" is\").str.replace(\"'t\", \" not\").str.replace(\"'m\", \" am\").str.replace(\"'ll\", \" will\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                    nigga just sign name not read nothing\n",
      "1                                                  now kia\n",
      "2        sure heat wave come game saturday weather peep...\n",
      "3                       kia really mature sound like voice\n",
      "4        never understand used car lot open hood car an...\n",
      "                               ...                        \n",
      "23907           new post electric kia niro sell month sale\n",
      "23908                                           thank much\n",
      "23909                            princess kia jahsjdj shut\n",
      "23910    chicago auto show preview chevrolet kia subaru...\n",
      "23911    kia dealer need take lesson lexus dealer custo...\n",
      "Name: clean_doc, Length: 23912, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(total_df['clean_doc'])\n",
    "print(type(total_df['clean_doc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#논문용\n",
    "#POS_DOC 열 생성: N과 J만 남김 or N과 J와 V만 남김\n",
    "total_df['nj_doc'] = total_df['clean_doc']\n",
    "total_df['njv_doc'] = total_df['clean_doc']\n",
    "tagged_list = []\n",
    "nj_list = []\n",
    "njv_list = []\n",
    "for i, doc in enumerate(total_df['clean_doc']):\n",
    "    tagged_list = pos_tag(doc.split())\n",
    "    nj_list = [t[0] for t in tagged_list if t[1].startswith('N') or t[1].startswith('J')]\n",
    "    njv_list = [t[0] for t in tagged_list if t[1].startswith('N') or t[1].startswith('J') or t[1].startswith('V')]\n",
    "    total_df['nj_doc'][i] = \" \".join(nj_list)\n",
    "    total_df['njv_doc'][i] = \" \".join(njv_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터의 정제가 끝났습니다. 다시 첫번째 훈련용 뉴스만 출력하여 정제 전, 후에 어떤 차이가 있는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nigga just sign name not read nothing'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#논문용\n",
    "total_df['clean_doc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nigga just sign name not read nothing'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#논문용\n",
    "total_df['clean_doc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "      <th>nj_doc</th>\n",
       "      <th>njv_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nigga just sign name not read nothing</td>\n",
       "      <td>nigga just sign name not read nothing</td>\n",
       "      <td>name nothing</td>\n",
       "      <td>sign name read nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>now kia</td>\n",
       "      <td>now kia</td>\n",
       "      <td></td>\n",
       "      <td>kia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sure ? heat wave come game saturday . weather ...</td>\n",
       "      <td>sure heat wave come game saturday weather peep...</td>\n",
       "      <td>sure heat game saturday call whopping degree</td>\n",
       "      <td>sure heat wave come game saturday peep call wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kia really mature sound . like voice</td>\n",
       "      <td>kia really mature sound like voice</td>\n",
       "      <td>kia sound voice</td>\n",
       "      <td>kia mature sound voice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>never understand used car lot open hood car . ...</td>\n",
       "      <td>never understand used car lot open hood car an...</td>\n",
       "      <td>used car lot open hood car anyone interested h...</td>\n",
       "      <td>understand used car lot open hood car anyone i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0              nigga just sign name not read nothing   \n",
       "1                                            now kia   \n",
       "2  sure ? heat wave come game saturday . weather ...   \n",
       "3               kia really mature sound . like voice   \n",
       "4  never understand used car lot open hood car . ...   \n",
       "\n",
       "                                           clean_doc  \\\n",
       "0              nigga just sign name not read nothing   \n",
       "1                                            now kia   \n",
       "2  sure heat wave come game saturday weather peep...   \n",
       "3                 kia really mature sound like voice   \n",
       "4  never understand used car lot open hood car an...   \n",
       "\n",
       "                                              nj_doc  \\\n",
       "0                                       name nothing   \n",
       "1                                                      \n",
       "2       sure heat game saturday call whopping degree   \n",
       "3                                    kia sound voice   \n",
       "4  used car lot open hood car anyone interested h...   \n",
       "\n",
       "                                             njv_doc  \n",
       "0                             sign name read nothing  \n",
       "1                                                kia  \n",
       "2  sure heat wave come game saturday peep call wh...  \n",
       "3                             kia mature sound voice  \n",
       "4  understand used car lot open hood car anyone i...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#논문용\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 특수문자가 제거되었으며, if나 you와 같은 길이가 3이하인 단어가 제거된 것을 확인할 수 있습니다. 뿐만 아니라 대문자가 전부 소문자로 바뀌었습니다. 이제 뉴스 데이터에서 불용어를 제거합니다. 불용어를 제거하기 위해서 토큰화를 우선 수행합니다. 토큰화와 불용어 제거를 순차적으로 진행합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#논문용1\n",
    "tokenized_doc = total_df['nj_doc'].apply(lambda x: x.split()) # 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#논문용2\n",
    "tokenized2_doc = total_df['njv_doc'].apply(lambda x: x.split()) # 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#논문용3\n",
    "tokenized3_doc = total_df['clean_doc'].apply(lambda x: x.split()) #토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 첫번째 훈련용 뉴스를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'nothing']\n",
      "['sign', 'name', 'read', 'nothing']\n",
      "['nigga', 'just', 'sign', 'name', 'not', 'read', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "#논문용\n",
    "print(tokenized_doc[0])\n",
    "print(tokenized2_doc[0])\n",
    "print(tokenized3_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      [name, nothing]\n",
       "1                                                   []\n",
       "2    [sure, heat, game, saturday, call, whopping, d...\n",
       "3                                  [kia, sound, voice]\n",
       "4    [used, car, lot, open, hood, car, anyone, inte...\n",
       "Name: nj_doc, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#논문용\n",
    "tokenized_doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          [sign, name, read, nothing]\n",
       "1                                                [kia]\n",
       "2    [sure, heat, wave, come, game, saturday, peep,...\n",
       "3                          [kia, mature, sound, voice]\n",
       "4    [understand, used, car, lot, open, hood, car, ...\n",
       "Name: njv_doc, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#논문용\n",
    "tokenized2_doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [nigga, just, sign, name, not, read, nothing]\n",
       "1                                           [now, kia]\n",
       "2    [sure, heat, wave, come, game, saturday, weath...\n",
       "3            [kia, really, mature, sound, like, voice]\n",
       "4    [never, understand, used, car, lot, open, hood...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#논문용\n",
    "tokenized3_doc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존에 있었던 불용어에 속하던 your, about, just, that, will, after 단어들이 사라졌을 뿐만 아니라, 토큰화가 수행된 것을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF 행렬 만들기\n",
    "불용어 제거를 위해 토큰화 작업을 수행하였지만, TfidfVectorizer(TF-IDF 챕터 참고)는 기본적으로 토큰화가 되어있지 않은 텍스트 데이터를 입력으로 사용합니다. 그렇기 때문에 TfidfVectorizer를 사용해서 TF-IDF 행렬을 만들기 위해서 다시 토큰화 작업을 역으로 취소하는 작업을 수행해보도록 하겠습니다. 이를 역토큰화(Detokenization)라고 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#논문용\n",
    "# 역토큰화 (토큰화 작업을 역으로 되돌림)\n",
    "detokenized_doc = []\n",
    "for i in range(len(total_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "total_df['nj_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#논문용\n",
    "# 역토큰화 (토큰화 작업을 역으로 되돌림)\n",
    "detokenized_doc = []\n",
    "for i in range(len(total_df)):\n",
    "    t = ' '.join(tokenized2_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "total_df['njv_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#논문용\n",
    "# 역토큰화 (토큰화 작업을 역으로 되돌림)\n",
    "detokenized_doc = []\n",
    "for i in range(len(total_df)):\n",
    "    t = ' '.join(tokenized3_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "total_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역토큰화가 제대로 되었는지 다시 첫번째 훈련용 뉴스를 출력하여 확인해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name nothing'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#논문용\n",
    "total_df['nj_doc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sign name read nothing'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#논문용\n",
    "total_df['njv_doc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nigga just sign name not read nothing'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#논문용\n",
    "total_df['clean_doc'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정상적으로 불용어가 제거된 상태에서 역토큰화가 수행되었음을 확인할 수 있습니다. \n",
    "이제 사이킷런의 TfidfVectorizer를 통해 단어 1,000개에 대한 TF-IDF 행렬을 만들 것입니다. 물론 텍스트 데이터에 있는 모든 단어를 가지고 행렬을 만들 수는 있겠지만, 여기서는 1,000개의 단어로 제한하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23912, 1000)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#논문용\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000, # 상위 1,000개의 단어를 보존 \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(total_df['nj_doc'])\n",
    "X.shape # TF-IDF 행렬의 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23912, 1000)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#논문용\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000, # 상위 1,000개의 단어를 보존 \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "X2 = vectorizer.fit_transform(total_df['njv_doc'])\n",
    "X2.shape # TF-IDF 행렬의 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23912, 1000)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#논문용\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000, # 상위 1,000개의 단어를 보존 \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "X3 = vectorizer.fit_transform(total_df['clean_doc'])\n",
    "X3.shape # TF-IDF 행렬의 크기 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bigram만들기\n",
    "8. 단어 토큰화와 텍스트 클린업\n",
    "각 문장의 구두점과 불필요한 문자를 제거하여, 단어의 list로 토크나이징합시다. 이작업에는 Gensim의 simple_preprocess() 가 훌륭합니다. 추가적으로 나는 구두점을 제거하기 위해서 deacc=True 로 설정하였습니다\n",
    "http://www.engear.net/wp/topic-modeling-gensimpython/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef sent_to_words(sentences):\\n    for sentence in sentences:\\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) # deacc=True removes punctuations\\n \\ndata_words = list(sent_to_words(data))\\n \\nprint(data_words[:1])\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) # deacc=True removes punctuations\n",
    " \n",
    "data_words = list(sent_to_words(data))\n",
    " \n",
    "print(data_words[:1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Bigram과 Trigram 모델 만들기\n",
    "Bigram은 문서에서 자주 발생되는 2연 단어입니다. Tigram은 자주 발생하는 3연 단어입니다. 이 예에서는 ‘front_bumper’,’oil_leak’,’maryland_college_park’등이 있습니다. Gensim의 Phrases 모델은 bigram, trigram, quadgram 등 그 이상을 구현할 수 있습니다. Phrases에 대한 두가지 중요한 인수는 min_count및 임계값입니다. 이 매개변수의 값이 높을수록 단어가 바이그램으로 결합되는 것이 어렵습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Build the bigram and trigram models\\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)\\n \\n# Faster way to get a sentence clubbed as a trigram/bigram\\nbigram_mod = gensim.models.phrases.Phraser(bigram)\\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\\n \\n# See trigram example\\nprint(trigram_mod[bigram_mod[data_words[0]]])\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    " \n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    " \n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA 토픽모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 TF-IDF 행렬을 다수의 행렬로 분해해보도록 하겠습니다. 여기서는 사이킷런의 절단된 SVD(Truncated SVD)를 사용합니다. 절단된 SVD를 사용하면 차원을 축소할 수 있습니다. 원래 기존 뉴스 데이터가 20개의 뉴스 카테고리를 갖고있었기 때문에, 20개의 토픽을 가졌다고 가정하고 토픽 모델링을 시도해보겠습니다. 토픽의 숫자는 n_components의 파라미터로 지정이 가능합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 svd_model.componets_는 앞서 배운 LSA에서 VT에 해당됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-e4b0d6d34a7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvd_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.shape(svd_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확하게 토픽의 수 t × 단어의 수의 크기를 가지는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svd_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-3704619a3ebe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Topic %d:\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mget_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvd_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mterms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'svd_model' is not defined"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(svd_model.components_,terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 20개의 행의 각 1,000개의 열 중 가장 값이 큰 5개의 값을 찾아서 단어로 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 각 단어에 정수 인코딩을 하는 동시에, 각 뉴스에서의 단어의 빈도수를 기록해보겠습니다. 여기서는 각 단어를 (word_id, word_frequency)의 형태로 바꾸고자 합니다. word_id는 단어가 정수 인코딩된 값이고, word_frequency는 해당 뉴스에서의 해당 단어의 빈도수를 의미합니다. 이는 gensim의 corpora.Dictionary()를 사용하여 손쉽게 구할 수 있습니다. 전체 뉴스에 대해서 정수 인코딩을 수행하고, 두번째 뉴스를 출력해봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(tokenized_doc)\n",
    "corpus = [dictionary.doc2bow(text) for  text in tokenized_doc]\n",
    "print(corpus[0]) #수행된 결과에서 첫번째 뉴스 출력. 첫번째 문서의 인덱스는 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary2 = corpora.Dictionary(tokenized2_doc)\n",
    "corpus2 = [dictionary2.doc2bow(text) for  text in tokenized2_doc]\n",
    "print(corpus2[0]) #수행된 결과에서 첫번째 뉴스 출력. 첫번째 문서의 인덱스는 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (1856, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary3 = corpora.Dictionary(tokenized3_doc)\n",
    "corpus3 = [dictionary2.doc2bow(text) for  text in tokenized3_doc]\n",
    "print(corpus3[0]) #수행된 결과에서 첫번째 뉴스 출력. 첫번째 문서의 인덱스는 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 뉴스의 출력 결과를 봅시다. 위의 출력 결과 중에서 (7, 1)는 정수 인코딩이 66으로 할당된 단어가 첫번째 뉴스에서는 한 번 등장하였음을 의미합니다. 7이라는 값을 가지는 단어가 정수 인코딩이 되기 전에는 어떤 단어였는지 확인하여봅시다. 이는 dictionary[]에 기존 단어가 무엇인지 알고자하는 정수값을 입력하여 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sure\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[7]) #7번째 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree\n"
     ]
    }
   ],
   "source": [
    "print(dictionary2[7]) #7번째 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kia\n"
     ]
    }
   ],
   "source": [
    "print(dictionary3[7]) #7번째 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존에는 단어 'faith'이었음을 알 수 있습니다. 총 학습된 단어의 개수를 확인해보겠습니다. 이는 dictionary의 길이를 확인하면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23841"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25764"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26942"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 8,358개의 단어가 학습되었습니다. 이제 LDA 모델을 훈련시켜보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA 모델 훈련시키기\n",
    "기존의 뉴스 데이터가 총 20개의 카테고리를 가지고 있었으므로 토픽의 개수를 20으로 하여 LDA 모델을 학습시켜보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter 지정하기.\n",
    "https://coredottoday.github.io/2018/09/17/%EB%AA%A8%EB%8D%B8-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.087*\"kia\" + 0.024*\"commercial\" + 0.013*\"get\" + 0.012*\"make\" + 0.009*\"superbowl\" + 0.008*\"good\" + 0.008*\"car\" + 0.007*\"see\" + 0.007*\"know\" + 0.007*\"people\" + 0.007*\"think\" + 0.007*\"want\" + 0.006*\"love\" + 0.006*\"say\" + 0.005*\"year\" + 0.005*\"super\" + 0.005*\"work\" + 0.005*\"great\" + 0.005*\"give\" + 0.005*\"time\"')\n",
      "(1, '0.072*\"kia\" + 0.012*\"hai\" + 0.010*\"name\" + 0.008*\"tha\" + 0.006*\"blackpink\" + 0.005*\"day\" + 0.004*\"world\" + 0.004*\"hain\" + 0.004*\"thank\" + 0.004*\"tour\" + 0.003*\"aur\" + 0.003*\"area\" + 0.003*\"say\" + 0.003*\"bhi\" + 0.003*\"main\" + 0.003*\"come\" + 0.003*\"happy\" + 0.003*\"time\" + 0.002*\"manchester\" + 0.002*\"sir\"')\n",
      "(2, '0.098*\"kia\" + 0.021*\"car\" + 0.020*\"new\" + 0.014*\"look\" + 0.014*\"sportage\" + 0.012*\"drive\" + 0.011*\"soul\" + 0.008*\"price\" + 0.007*\"take\" + 0.006*\"press\" + 0.006*\"auto\" + 0.006*\"hyundai\" + 0.006*\"telluride\" + 0.006*\"show\" + 0.005*\"get\" + 0.005*\"suv\" + 0.005*\"refresh\" + 0.005*\"vehicle\" + 0.005*\"debut\" + 0.005*\"check\"')\n"
     ]
    }
   ],
   "source": [
    "#NJV doc\n",
    "import gensim\n",
    "NUM_TOPICS = 3 #20개의 토픽, k=20\n",
    "ldamodel2 = gensim.models.ldamodel.LdaModel(corpus2, num_topics = NUM_TOPICS, id2word=dictionary2, passes=15, iterations=1500)\n",
    "ldamodel2.save('./Topicmodel/{}_NJV_{}_15_gensim_model.gensim'.format(brand, NUM_TOPICS))\n",
    "topics2 = ldamodel2.print_topics(num_words=20)\n",
    "for topic in topics2:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-b028887c75ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mNUM_TOPICS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;31m#20개의 토픽, k=20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mldamodel9\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNUM_TOPICS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdictionary2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mldamodel9\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Topicmodel/{}_NJV_{}_15_gensim_model.gensim'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_TOPICS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtopics9\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mldamodel9\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    978\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m                     )\n\u001b[1;32m--> 980\u001b[1;33m                     \u001b[0mgammat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[1;34m(self, chunk, state)\u001b[0m\n\u001b[0;32m    740\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m         \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    743\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    694\u001b[0m                 \u001b[0mElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m                 \u001b[0mexpElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mElogthetad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m                 \u001b[0mphinorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpElogthetad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m                 \u001b[1;31m# If gamma hasn't changed much, we're done.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[0mmeanchange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlastgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#NJV doc\n",
    "import gensim\n",
    "NUM_TOPICS = 10 #20개의 토픽, k=20\n",
    "ldamodel9 = gensim.models.ldamodel.LdaModel(corpus2, num_topics = NUM_TOPICS, id2word=dictionary2, passes=15, iterations=1500)\n",
    "ldamodel9.save('./Topicmodel/{}_NJV_{}_15_gensim_model.gensim'.format(brand, NUM_TOPICS))\n",
    "topics9 = ldamodel9.print_topics(num_words=20)\n",
    "for topic in topics9:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.050*\"choose\" + 0.033*\"head\" + 0.026*\"replace\" + 0.024*\"just\" + 0.022*\"jason\" + 0.019*\"run\" + 0.017*\"drag\" + 0.016*\"remember\" + 0.014*\"ulsan\" + 0.014*\"coscentralauto\" + 0.014*\"clear\" + 0.014*\"ticket\" + 0.012*\"woman\" + 0.012*\"calculate\" + 0.012*\"veganism\" + 0.011*\"unfortunately\" + 0.011*\"many\" + 0.010*\"veganfood\" + 0.010*\"thank\" + 0.009*\"complaint\"')\n",
      "(1, '0.074*\"choose\" + 0.045*\"america\" + 0.041*\"many\" + 0.030*\"employee\" + 0.024*\"county\" + 0.021*\"fan\" + 0.020*\"pvap\" + 0.016*\"hodge\" + 0.014*\"much\" + 0.013*\"office\" + 0.012*\"minor\" + 0.012*\"concern\" + 0.011*\"bullish\" + 0.010*\"chose\" + 0.010*\"superbowlcommercials\" + 0.010*\"mgf\" + 0.010*\"appear\" + 0.009*\"offensive\" + 0.009*\"meat\" + 0.009*\"sope\"')\n",
      "(2, '0.109*\"choose\" + 0.061*\"concern\" + 0.054*\"know\" + 0.016*\"guhhhhh\" + 0.016*\"fit\" + 0.015*\"regulator\" + 0.014*\"venture\" + 0.014*\"broadway\" + 0.012*\"however\" + 0.012*\"iuinna\" + 0.011*\"don\" + 0.011*\"layoff\" + 0.010*\"feature\" + 0.010*\"life\" + 0.009*\"head\" + 0.008*\"capefear\" + 0.008*\"paqpaq\" + 0.008*\"thursdaymorning\" + 0.008*\"maybe\" + 0.008*\"person\"')\n",
      "(3, '0.095*\"choose\" + 0.069*\"mazda\" + 0.048*\"whitecap\" + 0.041*\"guess\" + 0.033*\"rain\" + 0.028*\"point\" + 0.026*\"another\" + 0.025*\"many\" + 0.025*\"maclaren\" + 0.018*\"sure\" + 0.017*\"ebit\" + 0.016*\"yoongi\" + 0.014*\"recklessly\" + 0.014*\"innocean\" + 0.013*\"valley\" + 0.012*\"american\" + 0.011*\"ulsan\" + 0.010*\"offensive\" + 0.010*\"armenian\" + 0.009*\"part\"')\n",
      "(4, '0.056*\"benz\" + 0.044*\"choose\" + 0.034*\"betterbyhyundai\" + 0.029*\"taste\" + 0.026*\"afb\" + 0.025*\"approves\" + 0.025*\"make\" + 0.016*\"pre\" + 0.015*\"aston\" + 0.015*\"await\" + 0.015*\"action\" + 0.015*\"commission\" + 0.014*\"panel\" + 0.013*\"sister\" + 0.013*\"head\" + 0.012*\"click\" + 0.011*\"honest\" + 0.011*\"urvan\" + 0.010*\"vapor\" + 0.010*\"possible\"')\n",
      "(5, '0.041*\"choose\" + 0.040*\"capitol\" + 0.029*\"canister\" + 0.024*\"major\" + 0.022*\"list\" + 0.021*\"pain\" + 0.019*\"embarrassing\" + 0.017*\"nfl\" + 0.015*\"file\" + 0.014*\"change\" + 0.014*\"behemoth\" + 0.014*\"allgreen\" + 0.013*\"call\" + 0.012*\"new\" + 0.012*\"car\" + 0.012*\"bro\" + 0.012*\"honestly\" + 0.011*\"lot\" + 0.011*\"hayden\" + 0.011*\"greenfield\"')\n",
      "(6, '0.171*\"choose\" + 0.044*\"ulsan\" + 0.036*\"instead\" + 0.025*\"sit\" + 0.025*\"sunroof\" + 0.025*\"benz\" + 0.023*\"club\" + 0.023*\"congratulation\" + 0.022*\"difficulty\" + 0.019*\"show\" + 0.015*\"head\" + 0.015*\"battle\" + 0.013*\"just\" + 0.013*\"look\" + 0.012*\"intend\" + 0.011*\"pain\" + 0.011*\"reason\" + 0.010*\"tinus\" + 0.009*\"capitol\" + 0.009*\"remember\"')\n",
      "(7, '0.090*\"choose\" + 0.056*\"remember\" + 0.028*\"commercial\" + 0.026*\"depend\" + 0.023*\"milk\" + 0.023*\"head\" + 0.022*\"unless\" + 0.018*\"curious\" + 0.018*\"heater\" + 0.018*\"flight\" + 0.016*\"date\" + 0.014*\"yon\" + 0.014*\"own\" + 0.013*\"elantrasport\" + 0.012*\"delivers\" + 0.011*\"tron\" + 0.011*\"sorry\" + 0.010*\"sunvisors\" + 0.009*\"investment\" + 0.008*\"chungun\"')\n",
      "(8, '0.076*\"hal\" + 0.069*\"kmbc\" + 0.033*\"insanity\" + 0.028*\"discuss\" + 0.026*\"choose\" + 0.024*\"worldcancerday\" + 0.023*\"behold\" + 0.021*\"arona\" + 0.020*\"maybe\" + 0.018*\"leaf\" + 0.017*\"epic\" + 0.016*\"type\" + 0.015*\"boundary\" + 0.013*\"cosmetic\" + 0.013*\"info\" + 0.012*\"cardone\" + 0.011*\"level\" + 0.011*\"purchase\" + 0.010*\"tour\" + 0.010*\"button\"')\n",
      "(9, '0.106*\"choose\" + 0.078*\"budget\" + 0.053*\"lagrand\" + 0.024*\"give\" + 0.021*\"email\" + 0.019*\"head\" + 0.017*\"title\" + 0.016*\"student\" + 0.016*\"bcoz\" + 0.015*\"woman\" + 0.015*\"soooooooooooope\" + 0.014*\"ferrari\" + 0.014*\"mean\" + 0.013*\"meet\" + 0.013*\"teambuilding\" + 0.012*\"truly\" + 0.012*\"and\" + 0.012*\"thank\" + 0.012*\"beautiful\" + 0.011*\"instagram\"')\n",
      "(10, '0.083*\"city\" + 0.053*\"jose\" + 0.040*\"choose\" + 0.029*\"none\" + 0.029*\"meat\" + 0.026*\"calculate\" + 0.025*\"offensive\" + 0.025*\"taste\" + 0.022*\"visit\" + 0.019*\"many\" + 0.018*\"vitamin\" + 0.018*\"overreact\" + 0.017*\"legal\" + 0.017*\"highunday\" + 0.016*\"instead\" + 0.016*\"sans\" + 0.015*\"cell\" + 0.014*\"picture\" + 0.014*\"betterbyhyundai\" + 0.014*\"chose\"')\n",
      "(11, '0.113*\"mine\" + 0.109*\"head\" + 0.103*\"partnership\" + 0.097*\"racist\" + 0.093*\"close\" + 0.079*\"bitch\" + 0.075*\"sticker\" + 0.070*\"sexist\" + 0.070*\"see\" + 0.009*\"toss\" + 0.005*\"gross\" + 0.003*\"import\" + 0.003*\"soooo\" + 0.003*\"bell\" + 0.002*\"dress\" + 0.002*\"govt\" + 0.002*\"steven\" + 0.002*\"groundhog\" + 0.002*\"importado\" + 0.002*\"wonderwall\"')\n",
      "(12, '0.092*\"choose\" + 0.059*\"injury\" + 0.048*\"free\" + 0.034*\"seat\" + 0.025*\"sit\" + 0.025*\"mom\" + 0.025*\"fav\" + 0.024*\"ohio\" + 0.021*\"look\" + 0.017*\"economy\" + 0.016*\"veganism\" + 0.014*\"own\" + 0.014*\"entertain\" + 0.014*\"tirumalagiri\" + 0.012*\"employee\" + 0.011*\"prep\" + 0.011*\"check\" + 0.011*\"lender\" + 0.010*\"rosta\" + 0.010*\"night\"')\n",
      "(13, '0.105*\"taste\" + 0.101*\"prompt\" + 0.091*\"kristie\" + 0.075*\"choose\" + 0.040*\"guess\" + 0.034*\"action\" + 0.034*\"await\" + 0.025*\"multi\" + 0.025*\"today\" + 0.021*\"partnership\" + 0.016*\"tea\" + 0.015*\"whitecap\" + 0.014*\"hen\" + 0.013*\"spokesman\" + 0.009*\"accuse\" + 0.009*\"autom\" + 0.008*\"journey\" + 0.008*\"drives\" + 0.008*\"point\" + 0.008*\"single\"')\n",
      "(14, '0.080*\"several\" + 0.073*\"head\" + 0.054*\"santa\" + 0.045*\"russia\" + 0.041*\"choose\" + 0.026*\"dinner\" + 0.023*\"partnership\" + 0.023*\"ulsan\" + 0.019*\"pop\" + 0.019*\"mpg\" + 0.018*\"otherwise\" + 0.017*\"spotlight\" + 0.016*\"safety\" + 0.012*\"season\" + 0.010*\"incentive\" + 0.009*\"market\" + 0.009*\"fix\" + 0.008*\"trunk\" + 0.008*\"announces\" + 0.008*\"snow\"')\n",
      "(15, '0.147*\"guess\" + 0.116*\"choose\" + 0.073*\"february\" + 0.069*\"genesisautoleasing\" + 0.037*\"complaint\" + 0.034*\"whitecap\" + 0.032*\"bro\" + 0.030*\"kmbc\" + 0.027*\"offensive\" + 0.019*\"jose\" + 0.017*\"take\" + 0.015*\"dealership\" + 0.015*\"trailer\" + 0.015*\"full\" + 0.015*\"city\" + 0.012*\"unlike\" + 0.011*\"strand\" + 0.010*\"dct\" + 0.010*\"insanity\" + 0.008*\"many\"')\n",
      "(16, '0.150*\"choose\" + 0.072*\"official\" + 0.040*\"seoul\" + 0.036*\"feature\" + 0.035*\"chose\" + 0.034*\"history\" + 0.031*\"jungkook\" + 0.030*\"nominate\" + 0.023*\"founder\" + 0.022*\"enough\" + 0.020*\"sante\" + 0.020*\"support\" + 0.014*\"fully\" + 0.014*\"minjoonjin\" + 0.014*\"owner\" + 0.013*\"poster\" + 0.012*\"injury\" + 0.012*\"everyone\" + 0.011*\"manus\" + 0.011*\"nothing\"')\n",
      "(17, '0.068*\"choose\" + 0.033*\"valey\" + 0.032*\"carplay\" + 0.031*\"rise\" + 0.023*\"sparkling\" + 0.022*\"leather\" + 0.020*\"onditions\" + 0.018*\"ambassador\" + 0.018*\"random\" + 0.018*\"vivo\" + 0.017*\"require\" + 0.015*\"product\" + 0.015*\"glen\" + 0.013*\"autos\" + 0.013*\"face\" + 0.012*\"abuja\" + 0.012*\"suzuki\" + 0.012*\"survive\" + 0.012*\"plant\" + 0.011*\"elementary\"')\n"
     ]
    }
   ],
   "source": [
    "#Clean doc\n",
    "import gensim\n",
    "NUM_TOPICS = 18 #20개의 토픽, k=20\n",
    "ldamodel3 = gensim.models.ldamodel.LdaModel(corpus3, num_topics = NUM_TOPICS, id2word=dictionary3, passes=15, iterations=1500)\n",
    "ldamodel3.save('clean_18_15_gensim_model.gensim')\n",
    "topics3 = ldamodel3.print_topics(num_words=20)\n",
    "for topic in topics3:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pass 갯수 구하기\n",
    "http://www.engear.net/wp/topic-modeling-gensimpython/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 3.731863498687744\n",
      "Coherence -9.307217523311566\n",
      "Perplexity:  -8.385366060568076 \n",
      "\n",
      "\n",
      "epoch 5 14.143043994903564\n",
      "Coherence -9.44502498048043\n",
      "Perplexity:  -8.054349140493292 \n",
      "\n",
      "\n",
      "epoch 10 29.60848832130432\n",
      "Coherence -8.617723308852892\n",
      "Perplexity:  -8.044928884478006 \n",
      "\n",
      "\n",
      "epoch 15 46.249239683151245\n",
      "Coherence -8.971902951538556\n",
      "Perplexity:  -7.9809728271906595 \n",
      "\n",
      "\n",
      "epoch 20 58.115013122558594\n",
      "Coherence -9.79474996175303\n",
      "Perplexity:  -7.959344966593278 \n",
      "\n",
      "\n",
      "epoch 25 73.51534748077393\n",
      "Coherence -9.108365262972255\n",
      "Perplexity:  -7.923483042112234 \n",
      "\n",
      "\n",
      "epoch 30 87.76434254646301\n",
      "Coherence -9.612262728338182\n",
      "Perplexity:  -7.876465561015016 \n",
      "\n",
      "\n",
      "epoch 35 96.17158651351929\n",
      "Coherence -8.648830280642976\n",
      "Perplexity:  -7.841626122944669 \n",
      "\n",
      "\n",
      "epoch 40 114.56618475914001\n",
      "Coherence -8.628443204123913\n",
      "Perplexity:  -7.85100157988899 \n",
      "\n",
      "\n",
      "epoch 45 128.52831387519836\n",
      "Coherence -8.81643475013143\n",
      "Perplexity:  -7.819234579729832 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "import gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "coherences=[]\n",
    "perplexities=[]\n",
    "passes=[]\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    ntopics, nwords = 10, 20\n",
    "    if i==0:\n",
    "        p=1\n",
    "    else:\n",
    "        p=i*5\n",
    "    tic = time.time()\n",
    "    lda4 = gensim.models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=ntopics, iterations=1500, passes=p)\n",
    "    print('epoch',p,time.time() - tic)\n",
    "    # tfidf, corpus 무슨 차이?\n",
    "    # lda = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=ntopics, iterations=200000)\n",
    "\n",
    "    cm = CoherenceModel(model=lda4, corpus=corpus, coherence='u_mass')\n",
    "    coherence = cm.get_coherence()\n",
    "    print(\"Coherence\",coherence)\n",
    "    coherences.append(coherence)\n",
    "    print('Perplexity: ', lda4.log_perplexity(corpus),'\\n\\n')\n",
    "    perplexities.append(lda4.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-caaf35fe8d41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msearch_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbest_lda_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'search_params' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = GridSearchCV(gensim.models.ldamodel.LdaModel, param_grid=search_params)\n",
    "best_lda_model = model.best_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최적 토픽 갯수 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntopics 1 35.27652025222778\n",
      "Coherence:  0.8136872565926516\n",
      "Perplexity:  -8.172745002348243 \n",
      "\n",
      "\n",
      "ntopics 2 86.49971866607666\n",
      "Coherence:  0.7841100322944545\n",
      "Perplexity:  -8.160607400923114 \n",
      "\n",
      "\n",
      "ntopics 3 82.90421962738037\n",
      "Coherence:  0.7775720725620836\n",
      "Perplexity:  -8.178791548698399 \n",
      "\n",
      "\n",
      "ntopics 4 77.5515079498291\n",
      "Coherence:  0.7550291960376394\n",
      "Perplexity:  -8.230395814848698 \n",
      "\n",
      "\n",
      "ntopics 5 76.09523296356201\n",
      "Coherence:  0.6768139837670305\n",
      "Perplexity:  -8.36248162669458 \n",
      "\n",
      "\n",
      "ntopics 6 73.94680452346802\n",
      "Coherence:  0.7523717840044662\n",
      "Perplexity:  -8.418719069084698 \n",
      "\n",
      "\n",
      "ntopics 7 74.80311346054077\n",
      "Coherence:  0.6874649155620319\n",
      "Perplexity:  -8.46291430343537 \n",
      "\n",
      "\n",
      "ntopics 8 71.74015069007874\n",
      "Coherence:  0.7342381283264434\n",
      "Perplexity:  -8.666189445678885 \n",
      "\n",
      "\n",
      "ntopics 9 80.93096590042114\n",
      "Coherence:  0.6899096303588732\n",
      "Perplexity:  -8.915297184975033 \n",
      "\n",
      "\n",
      "ntopics 10 79.07452440261841\n",
      "Coherence:  0.705877535719794\n",
      "Perplexity:  -9.261616632004369 \n",
      "\n",
      "\n",
      "ntopics 11 78.94440269470215\n",
      "Coherence:  0.6971786931707669\n",
      "Perplexity:  -9.652505817962567 \n",
      "\n",
      "\n",
      "ntopics 12 83.57309579849243\n",
      "Coherence:  0.6886051047019116\n",
      "Perplexity:  -10.190504853082945 \n",
      "\n",
      "\n",
      "ntopics 13 76.55888080596924\n",
      "Coherence:  0.6995583678914671\n",
      "Perplexity:  -10.745429964198085 \n",
      "\n",
      "\n",
      "ntopics 14 77.87578988075256\n",
      "Coherence:  0.6838351692479087\n",
      "Perplexity:  -11.201996511001637 \n",
      "\n",
      "\n",
      "ntopics 15 70.62857270240784\n",
      "Coherence:  0.6865091131117621\n",
      "Perplexity:  -11.688528652610938 \n",
      "\n",
      "\n",
      "ntopics 16 72.19257354736328\n",
      "Coherence:  0.6844223923450523\n",
      "Perplexity:  -12.036425829371248 \n",
      "\n",
      "\n",
      "ntopics 17 70.69873118400574\n",
      "Coherence:  0.6940046530893318\n",
      "Perplexity:  -12.295798108016074 \n",
      "\n",
      "\n",
      "ntopics 18 70.85784411430359\n",
      "Coherence:  0.6979002043352414\n",
      "Perplexity:  -12.583138086595955 \n",
      "\n",
      "\n",
      "ntopics 19 71.50373268127441\n",
      "Coherence:  0.6928695294545598\n",
      "Perplexity:  -12.861776756206426 \n",
      "\n",
      "\n",
      "ntopics 20 71.0827534198761\n",
      "Coherence:  0.6894569967706321\n",
      "Perplexity:  -13.103958890310397 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import time\n",
    "import gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "coherencesNJV=[]\n",
    "perplexitiesNJV=[]\n",
    "passes=[]\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i in range(20):\n",
    "    if i==0:\n",
    "        ntopics = 1\n",
    "    else:\n",
    "        ntopics = (i+1)\n",
    "    nwords = 100\n",
    "    tic = time.time()\n",
    "    lda5 = gensim.models.ldamodel.LdaModel(corpus, id2word=dictionary2, num_topics=ntopics, iterations=400, passes=15)\n",
    "    print('ntopics',ntopics,time.time() - tic)\n",
    "\n",
    "    #cm = CoherenceModel(model=lda5, corpus=corpus, coherence='u_mass')\n",
    "    cm = CoherenceModel(model=lda5, texts = tokenized2_doc, dictionary = dictionary2, coherence='c_v')\n",
    "    coherence = cm.get_coherence()\n",
    "    print(\"Coherence: \",coherence)\n",
    "    coherencesNJV.append(coherence)\n",
    "    print('Perplexity: ', lda5.log_perplexity(corpus),'\\n\\n')\n",
    "    perplexitiesNJV.append(lda5.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8136872565926516, 0.7841100322944545, 0.7775720725620836, 0.7550291960376394, 0.6768139837670305, 0.7523717840044662, 0.6874649155620319, 0.7342381283264434, 0.6899096303588732, 0.705877535719794, 0.6971786931707669, 0.6886051047019116, 0.6995583678914671, 0.6838351692479087, 0.6865091131117621, 0.6844223923450523, 0.6940046530893318, 0.6979002043352414, 0.6928695294545598, 0.6894569967706321]\n",
      "[-8.172745002348243, -8.16060740043119, -8.178791892133857, -8.230396749047863, -8.362480311259532, -8.4187078881936, -8.462934495643976, -8.666209881474348, -8.915219538067085, -9.26167334282015, -9.652510712558467, -10.190439141679049, -10.74538073259766, -11.202032014087091, -11.688480172965523, -12.036439727662975, -12.295771601940352, -12.583124662115091, -12.86182676671815, -13.103863609606217]\n"
     ]
    }
   ],
   "source": [
    "print(coherencesNJV)\n",
    "print(perplexitiesNJV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA 시각화 하기\n",
    "LDA 시각화를 위해서는 pyLDAvis의 설치가 필요합니다. 윈도우의 명령 프롬프트나 MAC/UNIX의 터미널에서 아래의 명령을 수행하여 pyLDAvis를 설치하시기 바랍니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pyLDAvis\n",
    "설치가 완료되었다면 LDA 시각화 실습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el2186814422354224089056284646\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el2186814422354224089056284646_data = {\"mdsDat\": {\"x\": [-0.06205558109193877, -0.1698296814783422, 0.231885262570281], \"y\": [-0.20755244208830204, 0.1518692317411151, 0.055683210347186846], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [39.20891571044922, 30.507177352905273, 30.283905029296875]}, \"tinfo\": {\"Term\": [\"commercial\", \"new\", \"car\", \"look\", \"sportage\", \"make\", \"drive\", \"hai\", \"soul\", \"name\", \"superbowl\", \"price\", \"tha\", \"people\", \"want\", \"press\", \"hyundai\", \"blackpink\", \"auto\", \"super\", \"work\", \"give\", \"suv\", \"refresh\", \"love\", \"debut\", \"know\", \"bowl\", \"check\", \"vehicle\", \"commercial\", \"make\", \"superbowl\", \"want\", \"super\", \"work\", \"bowl\", \"give\", \"kid\", \"georgia\", \"point\", \"town\", \"feel\", \"superbowlads\", \"big\", \"west\", \"tell\", \"fuck\", \"american\", \"everything\", \"let\", \"night\", \"shit\", \"people\", \"plant\", \"job\", \"something\", \"man\", \"sure\", \"someone\", \"watch\", \"love\", \"game\", \"thing\", \"know\", \"think\", \"great\", \"see\", \"good\", \"way\", \"get\", \"kia\", \"say\", \"time\", \"year\", \"buy\", \"lol\", \"car\", \"telluride\", \"hai\", \"name\", \"tha\", \"blackpink\", \"hain\", \"tour\", \"aur\", \"bhi\", \"area\", \"main\", \"thank\", \"manchester\", \"nhi\", \"happy\", \"ticket\", \"nahi\", \"hota\", \"waitangi\", \"date\", \"sath\", \"mein\", \"kuch\", \"army\", \"due\", \"thi\", \"koi\", \"hua\", \"player\", \"demand\", \"log\", \"sir\", \"world\", \"day\", \"kia\", \"follow\", \"tweet\", \"please\", \"say\", \"come\", \"time\", \"friend\", \"pakistan\", \"get\", \"new\", \"sportage\", \"soul\", \"price\", \"press\", \"look\", \"debut\", \"refresh\", \"chicago\", \"forte\", \"sorento\", \"sale\", \"electric\", \"niro\", \"hot\", \"source\", \"rio\", \"releasesandutm\", \"trim\", \"check\", \"mild\", \"sell\", \"optimum\", \"mile\", \"model\", \"toyota\", \"ford\", \"review\", \"front\", \"stinger\", \"drive\", \"hyundai\", \"suv\", \"roadshow\", \"vehicle\", \"auto\", \"car\", \"kia\", \"take\", \"change\", \"show\", \"telluride\", \"get\", \"brand\"], \"Freq\": [2048.0, 1290.0, 2021.0, 912.0, 886.0, 1021.0, 804.0, 767.0, 699.0, 661.0, 798.0, 541.0, 512.0, 587.0, 558.0, 420.0, 397.0, 391.0, 426.0, 459.0, 457.0, 442.0, 354.0, 334.0, 573.0, 318.0, 677.0, 405.0, 314.0, 338.0, 2048.12060546875, 1021.092041015625, 797.9100952148438, 557.9576416015625, 458.73333740234375, 456.4889831542969, 404.4976806640625, 441.3109130859375, 369.2785949707031, 333.6893005371094, 347.17315673828125, 276.5889892578125, 307.8951416015625, 238.826416015625, 329.1537170410156, 261.22674560546875, 259.61041259765625, 222.08836364746094, 220.47039794921875, 210.8436737060547, 210.2606201171875, 187.55393981933594, 178.1012725830078, 584.9946899414062, 167.8138885498047, 190.43064880371094, 186.5601348876953, 152.3605499267578, 147.65805053710938, 149.94735717773438, 249.57839965820312, 538.910888671875, 269.1796569824219, 336.614501953125, 597.2268676757812, 561.2808227539062, 442.12164306640625, 606.7928466796875, 642.7228393554688, 285.3896179199219, 1102.0069580078125, 7394.7880859375, 496.0823974609375, 418.79296875, 463.2666931152344, 366.66900634765625, 303.49517822265625, 639.9840087890625, 352.91729736328125, 766.5729370117188, 660.8509521484375, 511.81005859375, 390.6923522949219, 246.55252075195312, 235.14822387695312, 225.75192260742188, 220.43504333496094, 223.2560577392578, 202.14102172851562, 239.4325408935547, 160.63092041015625, 150.4442596435547, 180.41433715820312, 136.96697998046875, 116.07160186767578, 115.54132843017578, 119.9441146850586, 150.04052734375, 111.32816314697266, 109.7415542602539, 104.66173553466797, 106.52973175048828, 121.52687072753906, 98.65323638916016, 98.46694946289062, 98.91015625, 104.91405487060547, 116.10557556152344, 93.15714263916016, 158.527587890625, 268.06005859375, 343.9893493652344, 4734.0322265625, 156.5782470703125, 139.38360595703125, 151.31044006347656, 220.9592742919922, 199.62045288085938, 165.42063903808594, 136.2162628173828, 123.82532501220703, 146.99639892578125, 1289.6865234375, 885.7317504882812, 699.0308837890625, 540.986083984375, 419.80218505859375, 910.7535400390625, 317.47833251953125, 333.3375244140625, 290.30987548828125, 283.4544982910156, 275.06573486328125, 269.84033203125, 241.91439819335938, 232.7595977783203, 246.92869567871094, 223.33172607421875, 219.88780212402344, 205.07508850097656, 203.0922393798828, 313.66064453125, 204.33445739746094, 209.81358337402344, 169.75625610351562, 179.78759765625, 179.5400848388672, 162.68106079101562, 166.48312377929688, 153.05416870117188, 153.26113891601562, 148.0443115234375, 795.6735229492188, 395.90869140625, 350.6505432128906, 168.9627227783203, 328.60540771484375, 403.20635986328125, 1381.2928466796875, 6407.49951171875, 437.5625305175781, 260.8232727050781, 372.2315979003906, 395.6097412109375, 359.2396545410156, 228.52186584472656], \"Total\": [2048.0, 1290.0, 2021.0, 912.0, 886.0, 1021.0, 804.0, 767.0, 699.0, 661.0, 798.0, 541.0, 512.0, 587.0, 558.0, 420.0, 397.0, 391.0, 426.0, 459.0, 457.0, 442.0, 354.0, 334.0, 573.0, 318.0, 677.0, 405.0, 314.0, 338.0, 2048.718994140625, 1021.7442016601562, 798.507568359375, 558.654052734375, 459.3873291015625, 457.1581726074219, 405.11663818359375, 442.0162353515625, 369.9397888183594, 334.29339599609375, 347.8039855957031, 277.1998291015625, 308.61053466796875, 239.4239044189453, 329.9779968261719, 261.8928527832031, 260.2991027832031, 222.7015380859375, 221.0857391357422, 211.4715118408203, 210.96018981933594, 188.1924285888672, 178.71966552734375, 587.1174926757812, 168.42340087890625, 191.12939453125, 187.2855224609375, 153.0125732421875, 148.30589294433594, 150.63516235351562, 252.09376525878906, 573.7575073242188, 278.8580017089844, 358.4590148925781, 677.67724609375, 633.0077514648438, 499.29150390625, 727.385009765625, 790.0072631835938, 305.492919921875, 1608.2430419921875, 18536.3203125, 717.398681640625, 584.5482177734375, 703.678466796875, 482.6048278808594, 361.9394836425781, 2021.58837890625, 748.8367919921875, 767.177490234375, 661.6248168945312, 512.4136352539062, 391.2950439453125, 247.1529541015625, 235.76983642578125, 226.3553009033203, 221.037109375, 223.95590209960938, 202.77537536621094, 240.1845703125, 161.24461364746094, 151.04522705078125, 181.22830200195312, 137.58721923828125, 116.67289733886719, 116.14521026611328, 120.57304382324219, 150.84701538085938, 111.93164825439453, 110.34204864501953, 105.26216125488281, 107.14735412597656, 122.24826049804688, 99.25580596923828, 99.07418060302734, 99.52067565917969, 105.5661392211914, 116.83042907714844, 93.76207733154297, 160.2390899658203, 316.56781005859375, 526.9859008789062, 18536.3203125, 187.44717407226562, 177.04478454589844, 264.46649169921875, 717.398681640625, 603.1354370117188, 584.5482177734375, 259.7734680175781, 168.35733032226562, 1608.2430419921875, 1290.36474609375, 886.3901977539062, 699.6863403320312, 541.6192626953125, 420.4299621582031, 912.4314575195312, 318.1155090332031, 334.015380859375, 290.9416809082031, 284.0843200683594, 275.6953125, 270.4957580566406, 242.54861450195312, 233.38824462890625, 247.5994415283203, 223.9747314453125, 220.53524780273438, 205.69602966308594, 203.71412658691406, 314.62615966796875, 205.03414916992188, 210.53968811035156, 170.39073181152344, 180.460205078125, 180.2197723388672, 163.32833862304688, 167.16969299316406, 153.6853485107422, 153.9322509765625, 148.69351196289062, 804.1036987304688, 397.8784484863281, 354.24267578125, 169.732666015625, 338.80517578125, 426.1327819824219, 2021.58837890625, 18536.3203125, 671.1978759765625, 314.91986083984375, 590.3556518554688, 748.8367919921875, 1608.2430419921875, 303.0770263671875], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.7227001190185547, -4.418700218200684, -4.66540002822876, -5.023099899291992, -5.218900203704834, -5.223800182342529, -5.344699859619141, -5.257599830627441, -5.435800075531006, -5.537099838256836, -5.497499942779541, -5.724800109863281, -5.617599964141846, -5.871600151062012, -5.55079984664917, -5.7820000648498535, -5.7881999015808105, -5.944300174713135, -5.951600074768066, -5.996200084686279, -5.999000072479248, -6.11329984664917, -6.164999961853027, -4.9756999015808105, -6.2245001792907715, -6.098100185394287, -6.118599891662598, -6.321100234985352, -6.352499961853027, -6.337100028991699, -5.827600002288818, -5.057799816131592, -5.751999855041504, -5.52839994430542, -4.955100059509277, -5.017099857330322, -5.255799770355225, -4.939199924468994, -4.8815999031066895, -5.69350004196167, -4.34250020980835, -2.438800096511841, -5.140600204467773, -5.309999942779541, -5.209000110626221, -5.44290018081665, -5.631999969482422, -4.885900020599365, -5.481100082397461, -4.454500198364258, -4.60290002822876, -4.858500003814697, -5.128499984741211, -5.588799953460693, -5.636199951171875, -5.677000045776367, -5.700799942016602, -5.6880998611450195, -5.787399768829346, -5.618100166320801, -6.017300128936768, -6.082799911499023, -5.901199817657471, -6.176700115203857, -6.342199802398682, -6.346799850463867, -6.3094000816345215, -6.0854997634887695, -6.383900165557861, -6.3983001708984375, -6.445700168609619, -6.427999973297119, -6.296299934387207, -6.504799842834473, -6.506700038909912, -6.502200126647949, -6.443299770355225, -6.341899871826172, -6.562099933624268, -6.0304999351501465, -5.505199909210205, -5.255799770355225, -2.6338999271392822, -6.042900085449219, -6.159200191497803, -6.077099800109863, -5.698400020599365, -5.800000190734863, -5.9878997802734375, -6.182199954986572, -6.277500152587891, -6.105999946594238, -3.9268999099731445, -4.302700042724609, -4.539400100708008, -4.7957000732421875, -5.049300193786621, -4.274799823760986, -5.328700065612793, -5.279900074005127, -5.418099880218506, -5.441999912261963, -5.472099781036377, -5.491199970245361, -5.600500106811523, -5.639100074768066, -5.579999923706055, -5.6803998947143555, -5.695899963378906, -5.765699863433838, -5.775400161743164, -5.340799808502197, -5.7692999839782715, -5.742800235748291, -5.954699993133545, -5.897299766540527, -5.89870023727417, -5.997300148010254, -5.9741997718811035, -6.058300018310547, -6.0569000244140625, -6.091599941253662, -4.409900188446045, -5.107900142669678, -5.229300022125244, -5.959400177001953, -5.2941999435424805, -5.089600086212158, -3.858299970626831, -2.3238000869750977, -5.007800102233887, -5.525199890136719, -5.16949987411499, -5.10860013961792, -5.205100059509277, -5.657400131225586], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9359999895095825, 0.9355999827384949, 0.9355000257492065, 0.9350000023841858, 0.9348000288009644, 0.9348000288009644, 0.9347000122070312, 0.9347000122070312, 0.934499979019165, 0.934499979019165, 0.934499979019165, 0.9340999722480774, 0.933899998664856, 0.9337999820709229, 0.9337999820709229, 0.9337000250816345, 0.9336000084877014, 0.9334999918937683, 0.9334999918937683, 0.9333000183105469, 0.9329000115394592, 0.9329000115394592, 0.9327999949455261, 0.9326000213623047, 0.9326000213623047, 0.9326000213623047, 0.9323999881744385, 0.9319999814033508, 0.9319000244140625, 0.9316999912261963, 0.9261999726295471, 0.8736000061035156, 0.9009000062942505, 0.8733999729156494, 0.8098999857902527, 0.8159999847412109, 0.8147000074386597, 0.7549999952316284, 0.7299000024795532, 0.8682000041007996, 0.5583000183105469, 0.01730000041425228, 0.5673999786376953, 0.6028000116348267, 0.5181999802589417, 0.6614999771118164, 0.760200023651123, -0.21389999985694885, 0.18400000035762787, 1.186400055885315, 1.1859999895095825, 1.1859999895095825, 1.1857000589370728, 1.1848000288009644, 1.1845999956130981, 1.184499979019165, 1.184499979019165, 1.1841000318527222, 1.1841000318527222, 1.1841000318527222, 1.18340003490448, 1.1832000017166138, 1.1827000379562378, 1.1827000379562378, 1.1820000410079956, 1.1820000410079956, 1.1820000410079956, 1.1818000078201294, 1.1818000078201294, 1.1818000078201294, 1.18149995803833, 1.181399941444397, 1.1813000440597534, 1.1811000108718872, 1.1811000108718872, 1.1811000108718872, 1.180999994277954, 1.180999994277954, 1.1806999444961548, 1.1764999628067017, 1.020900011062622, 0.7605999708175659, -0.1776999980211258, 1.0073000192642212, 0.9480000138282776, 0.6287999749183655, 0.009600000455975533, 0.08150000125169754, -0.07509999722242355, 0.5415999889373779, 0.8799999952316284, -1.205299973487854, 1.194000005722046, 1.1937999725341797, 1.193600058555603, 1.1934000253677368, 1.1930999755859375, 1.1927000284194946, 1.1924999952316284, 1.1924999952316284, 1.1923999786376953, 1.1922999620437622, 1.1922999620437622, 1.1921000480651855, 1.1919000148773193, 1.1919000148773193, 1.1917999982833862, 1.1916999816894531, 1.19159996509552, 1.191499948501587, 1.191499948501587, 1.191499948501587, 1.191100001335144, 1.191100001335144, 1.1907999515533447, 1.1907999515533447, 1.1907999515533447, 1.190600037574768, 1.1904000043869019, 1.1904000043869019, 1.1901999711990356, 1.1901999711990356, 1.184000015258789, 1.1895999908447266, 1.184399962425232, 1.190000057220459, 1.1640000343322754, 1.139299988746643, 0.8137000203132629, 0.13230000436306, 0.766700029373169, 1.006100058555603, 0.733299970626831, 0.5565000176429749, -0.3043999969959259, 0.9121999740600586]}, \"token.table\": {\"Topic\": [1, 2, 2, 2, 1, 3, 2, 1, 2, 1, 1, 3, 1, 3, 1, 3, 2, 3, 2, 3, 3, 1, 2, 3, 1, 2, 1, 2, 3, 3, 2, 1, 3, 2, 3, 1, 1, 1, 2, 3, 3, 3, 1, 2, 3, 1, 1, 2, 1, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 2, 2, 2, 3, 2, 2, 1, 3, 1, 1, 2, 3, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 3, 1, 2, 2, 1, 1, 2, 2, 3, 3, 3, 2, 2, 3, 2, 1, 3, 3, 1, 2, 1, 2, 1, 2, 1, 2, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 1, 2, 1, 2, 3, 3, 1, 1, 2, 3, 1, 2, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 3, 1, 2, 3, 1, 1, 3, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 3, 3, 1, 2, 1, 3, 2, 1, 1, 3, 1, 2, 1, 1, 2, 3, 1, 2, 3], \"Freq\": [0.995089054107666, 0.9957317113876343, 0.9986247420310974, 0.9984303116798401, 0.053973786532878876, 0.9457145929336548, 0.995307981967926, 0.9970361590385437, 0.9992460012435913, 0.9972436428070068, 0.24416235089302063, 0.7555835247039795, 0.7604565620422363, 0.240362286567688, 0.31658273935317993, 0.6831262111663818, 0.1714721918106079, 0.8287822604179382, 0.00317837530747056, 0.9980098009109497, 0.9967633485794067, 0.47584667801856995, 0.33160048723220825, 0.19398628175258636, 0.9996490478515625, 0.9943849444389343, 0.28463760018348694, 0.6527689099311829, 0.06262027472257614, 0.996493399143219, 0.9928920269012451, 0.009948965162038803, 0.989922046661377, 0.9979692101478577, 0.9977381229400635, 0.9977703094482422, 0.99802166223526, 0.15471024811267853, 0.8375692963600159, 0.01066967286169529, 0.9930029511451721, 0.9961830973625183, 0.4734894633293152, 0.523533046245575, 0.9939437508583069, 0.9968498945236206, 0.9646486639976501, 0.03227449208498001, 0.9991223216056824, 0.6852198243141174, 0.09140409529209137, 0.2232249677181244, 0.9977009296417236, 0.8139165639877319, 0.048100825399160385, 0.13797341287136078, 0.8852543830871582, 0.016022704541683197, 0.10014189779758453, 0.9997686743736267, 0.9993811249732971, 0.9932223558425903, 0.9975789785385132, 0.9987497329711914, 0.9947681427001953, 0.005026660859584808, 0.9952788352966309, 0.9940909147262573, 0.39894649386405945, 0.2553904950618744, 0.3456457257270813, 0.9974595904350281, 0.8809503316879272, 0.11805029958486557, 0.9891577959060669, 0.9975094199180603, 0.9954484701156616, 0.991872251033783, 0.8371565341949463, 0.1602477878332138, 0.0010959727223962545, 0.9984311461448669, 0.9394212365150452, 0.061001379042863846, 0.9961761832237244, 0.9992716312408447, 0.9933823943138123, 0.9984829425811768, 0.9969000816345215, 0.9949561953544617, 0.9974498152732849, 0.9987805485725403, 0.9942325949668884, 0.9990556240081787, 0.9997173547744751, 0.9930800199508667, 0.9989774823188782, 0.9983364939689636, 0.9977068305015564, 0.2613488733768463, 0.7365286946296692, 0.9963933825492859, 0.0034064732026308775, 0.9974861145019531, 0.994637131690979, 0.3478701710700989, 0.5709608197212219, 0.07940514385700226, 0.9976884126663208, 0.9989773035049438, 0.9988566637039185, 0.9969601035118103, 0.9966162443161011, 0.9955405592918396, 0.9975729584693909, 0.9956834316253662, 0.9981672167778015, 0.9916766285896301, 0.6913868188858032, 0.30805742740631104, 0.8344961404800415, 0.009623514488339424, 0.15535101294517517, 0.9974366426467896, 0.9959732294082642, 0.2574719190597534, 0.11179701238870621, 0.6301286220550537, 0.006240674294531345, 0.9922672510147095, 0.9957834482192993, 0.9984754920005798, 0.9974779486656189, 0.9990190863609314, 0.9956480264663696, 0.9995598196983337, 0.9953359365463257, 0.9991568326950073, 0.9993643760681152, 0.9982295036315918, 0.9979374408721924, 0.008468770422041416, 0.9908461570739746, 0.2741367518901825, 0.07449368387460709, 0.6525646448135376, 0.99885094165802, 0.4713977873325348, 0.5288201570510864, 0.9991927742958069, 0.9950680732727051, 0.9974227547645569, 0.9401353597640991, 0.06137382239103317, 0.88624507188797, 0.11216292530298233, 0.9957320094108582, 0.7167928814888, 0.28226926922798157, 0.9967347979545593, 0.9992791414260864, 0.9979897141456604, 0.996494472026825, 0.20898666977882385, 0.7851120829582214, 0.02951548807322979, 0.9710595607757568, 0.9952473044395447, 0.9988292455673218, 0.9916945099830627, 0.007933556102216244, 0.9329184889793396, 0.065467968583107, 0.996590793132782, 0.9974665641784668, 0.8465800881385803, 0.15162628889083862, 0.6579709649085999, 0.12079380452632904, 0.2202710509300232], \"Term\": [\"american\", \"area\", \"army\", \"aur\", \"auto\", \"auto\", \"bhi\", \"big\", \"blackpink\", \"bowl\", \"brand\", \"brand\", \"buy\", \"buy\", \"car\", \"car\", \"change\", \"change\", \"check\", \"check\", \"chicago\", \"come\", \"come\", \"come\", \"commercial\", \"date\", \"day\", \"day\", \"day\", \"debut\", \"demand\", \"drive\", \"drive\", \"due\", \"electric\", \"everything\", \"feel\", \"follow\", \"follow\", \"follow\", \"ford\", \"forte\", \"friend\", \"friend\", \"front\", \"fuck\", \"game\", \"game\", \"georgia\", \"get\", \"get\", \"get\", \"give\", \"good\", \"good\", \"good\", \"great\", \"great\", \"great\", \"hai\", \"hain\", \"happy\", \"hot\", \"hota\", \"hua\", \"hyundai\", \"hyundai\", \"job\", \"kia\", \"kia\", \"kia\", \"kid\", \"know\", \"know\", \"koi\", \"kuch\", \"let\", \"log\", \"lol\", \"lol\", \"look\", \"look\", \"love\", \"love\", \"main\", \"make\", \"man\", \"manchester\", \"mein\", \"mild\", \"mile\", \"model\", \"nahi\", \"name\", \"new\", \"nhi\", \"night\", \"niro\", \"optimum\", \"pakistan\", \"pakistan\", \"people\", \"people\", \"plant\", \"player\", \"please\", \"please\", \"please\", \"point\", \"press\", \"price\", \"refresh\", \"releasesandutm\", \"review\", \"rio\", \"roadshow\", \"sale\", \"sath\", \"say\", \"say\", \"see\", \"see\", \"see\", \"sell\", \"shit\", \"show\", \"show\", \"show\", \"sir\", \"sir\", \"someone\", \"something\", \"sorento\", \"soul\", \"source\", \"sportage\", \"stinger\", \"super\", \"superbowl\", \"superbowlads\", \"sure\", \"suv\", \"suv\", \"take\", \"take\", \"take\", \"tell\", \"telluride\", \"telluride\", \"tha\", \"thank\", \"thi\", \"thing\", \"thing\", \"think\", \"think\", \"ticket\", \"time\", \"time\", \"tour\", \"town\", \"toyota\", \"trim\", \"tweet\", \"tweet\", \"vehicle\", \"vehicle\", \"waitangi\", \"want\", \"watch\", \"watch\", \"way\", \"way\", \"west\", \"work\", \"world\", \"world\", \"year\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el2186814422354224089056284646\", ldavis_el2186814422354224089056284646_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el2186814422354224089056284646\", ldavis_el2186814422354224089056284646_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el2186814422354224089056284646\", ldavis_el2186814422354224089056284646_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel2, corpus2, dictionary2) #sort_topics = False\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis ,'./토픽모델결과/pyLDAvis_{}.html'.format(brand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좌측의 원들은 각각의 20개의 토픽을 나타냅니다. 각 원과의 거리는 각 토픽들이 서로 얼마나 다른지를 보여줍니다. 만약 두 개의 원이 겹친다면, 이 두 개의 토픽은 유사한 토픽이라는 의미입니다. 위의 그림에서는 10번 토픽을 클릭하였고, 이에 따라 우측에는 10번 토픽에 대한 정보가 나타납니다. 한 가지 주의할 점은 LDA 모델의 출력 결과에서는 토픽 번호가 0부터 할당되어 0~19의 숫자가 사용된 것과는 달리 위의 LDA 시각화에서는 토픽의 번호가 1부터 시작하므로 각 토픽 번호는 이제 +1이 된 값인 1~20까지의 값을 가집니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 별 토픽 분포 보기\n",
    "위에서 토픽 별 단어 분포는 확인하였으나, 아직 문서 별 토픽 분포에 대해서는 확인하지 못 하였습니다. 우선 문서 별 토픽 분포를 확인하는 방법을 보겠습니다. 각 문서의 토픽 분포는 이미 훈련된 LDA 모델인 ldamodel[]에 전체 데이터가 정수 인코딩 된 결과를 넣은 후에 확인이 가능합니다. 여기서는 책의 지면의 한계로 상위 5개의 문서에 대해서만 토픽 분포를 확인해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 문서의 topic 비율은 [(0, 0.44358692), (1, 0.44528162), (2, 0.111131474)]\n",
      "1 번째 문서의 topic 비율은 [(0, 0.33333334), (1, 0.33333334), (2, 0.33333334)]\n",
      "2 번째 문서의 topic 비율은 [(0, 0.90424377), (1, 0.047396574), (2, 0.048359636)]\n",
      "3 번째 문서의 topic 비율은 [(0, 0.08512809), (1, 0.45860633), (2, 0.45626557)]\n",
      "4 번째 문서의 topic 비율은 [(0, 0.8354236), (1, 0.032398656), (2, 0.1321777)]\n",
      "5 번째 문서의 topic 비율은 [(0, 0.08725744), (1, 0.06739407), (2, 0.8453485)]\n",
      "6 번째 문서의 topic 비율은 [(0, 0.18512416), (1, 0.6273152), (2, 0.18756065)]\n",
      "7 번째 문서의 topic 비율은 [(0, 0.11117199), (1, 0.11117044), (2, 0.77765757)]\n",
      "8 번째 문서의 topic 비율은 [(0, 0.4426782), (1, 0.11364022), (2, 0.44368157)]\n",
      "9 번째 문서의 topic 비율은 [(0, 0.06667413), (1, 0.06714799), (2, 0.8661779)]\n"
     ]
    }
   ],
   "source": [
    "for i, topic_list in enumerate(ldamodel2[corpus]):\n",
    "    if i==10:\n",
    "        break\n",
    "    print(i,'번째 문서의 topic 비율은',topic_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 출력 결과에서 (숫자, 확률)은 각각 토픽 번호와 해당 토픽이 해당 문서에서 차지하는 분포도를 의미합니다. 예를 들어 0번째 문서의 토픽 비율에서 (7, 0.3050222)은 7번 토픽이 30%의 분포도를 가지는 것을 의미합니다. 위의 코드를 응용하여 좀 더 깔끔한 형태인 데이터프레임 형식으로 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topictable_per_doc(model, corpus, texts):\n",
    "    topic_table = pd.DataFrame()\n",
    "\n",
    "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n",
    "    for i, topic_list in enumerate(model[corpus]):\n",
    "        doc = topic_list[0] if model.per_word_topics else topic_list            \n",
    "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
    "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n",
    "        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n",
    "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n",
    "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n",
    "\n",
    "        # 모든 문서에 대해서 각각 아래를 수행\n",
    "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n",
    "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n",
    "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
    "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n",
    "            else:\n",
    "                break\n",
    "    return(topic_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문서 번호</th>\n",
       "      <th>가장 비중이 높은 토픽</th>\n",
       "      <th>가장 높은 토픽의 비중</th>\n",
       "      <th>각 토픽의 비중</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4643</td>\n",
       "      <td>[(0, 0.46254268), (1, 0.464349), (2, 0.07310836)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5825</td>\n",
       "      <td>[(0, 0.218196), (1, 0.19934238), (2, 0.5824616)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5925</td>\n",
       "      <td>[(0, 0.5924966), (1, 0.19493365), (2, 0.212569...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8617</td>\n",
       "      <td>[(0, 0.86169815), (1, 0.06872418), (2, 0.06957...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6007</td>\n",
       "      <td>[(0, 0.3723287), (1, 0.026954835), (2, 0.60071...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9021</td>\n",
       "      <td>[(0, 0.04904117), (1, 0.048830584), (2, 0.9021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7504</td>\n",
       "      <td>[(0, 0.7504), (1, 0.12561104), (2, 0.12398899)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7570</td>\n",
       "      <td>[(0, 0.75697047), (1, 0.11956066), (2, 0.12346...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5048</td>\n",
       "      <td>[(0, 0.40657148), (1, 0.088614166), (2, 0.5048...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8553</td>\n",
       "      <td>[(0, 0.07211964), (1, 0.85530597), (2, 0.07257...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   문서 번호  가장 비중이 높은 토픽  가장 높은 토픽의 비중  \\\n",
       "0      0           1.0        0.4643   \n",
       "1      1           2.0        0.5825   \n",
       "2      2           0.0        0.5925   \n",
       "3      3           0.0        0.8617   \n",
       "4      4           2.0        0.6007   \n",
       "5      5           2.0        0.9021   \n",
       "6      6           0.0        0.7504   \n",
       "7      7           0.0        0.7570   \n",
       "8      8           2.0        0.5048   \n",
       "9      9           1.0        0.8553   \n",
       "\n",
       "                                            각 토픽의 비중  \n",
       "0  [(0, 0.46254268), (1, 0.464349), (2, 0.07310836)]  \n",
       "1   [(0, 0.218196), (1, 0.19934238), (2, 0.5824616)]  \n",
       "2  [(0, 0.5924966), (1, 0.19493365), (2, 0.212569...  \n",
       "3  [(0, 0.86169815), (1, 0.06872418), (2, 0.06957...  \n",
       "4  [(0, 0.3723287), (1, 0.026954835), (2, 0.60071...  \n",
       "5  [(0, 0.04904117), (1, 0.048830584), (2, 0.9021...  \n",
       "6    [(0, 0.7504), (1, 0.12561104), (2, 0.12398899)]  \n",
       "7  [(0, 0.75697047), (1, 0.11956066), (2, 0.12346...  \n",
       "8  [(0, 0.40657148), (1, 0.088614166), (2, 0.5048...  \n",
       "9  [(0, 0.07211964), (1, 0.85530597), (2, 0.07257...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topictable = make_topictable_per_doc(ldamodel2, corpus2, tokenized2_doc)\n",
    "topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\n",
    "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n",
    "topictable[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23912"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topictable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(file, topictable, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('./토픽모델결과/병합토픽테이블_{}.csv'.format(brand), encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "topictable.to_csv('./토픽모델결과/토픽테이블_{}.csv'.format(brand), encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 비중이 높은 토픽\n",
      "0.0    10453\n",
      "1.0     6393\n",
      "2.0     7066\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "0.0\n",
      "6488\n"
     ]
    }
   ],
   "source": [
    "#필터링할 문서들의 문서번호 갖고오기\n",
    "filter_doc = []\n",
    "filtered_topic = 2.0\n",
    "print(topictable.groupby('가장 비중이 높은 토픽').size())\n",
    "#print(filter_doc)\n",
    "print(type(topictable))\n",
    "print(topictable['가장 비중이 높은 토픽'][3])\n",
    "with open('./NJ_topic5_2번_0.5이상문서번호.txt', 'w') as f:\n",
    "    for i in range(len(topictable)):\n",
    "        if topictable['가장 비중이 높은 토픽'][i] ==2.0 :\n",
    "            #filter_doc.append(topictable['문서 번호'][i])\n",
    "            if topictable['가장 높은 토픽의 비중'][i] >= 0.5 :  #걸러야 할것. 2.0이 너무 확률이 큰 것.\n",
    "                filter_doc.append(topictable['문서 번호'][i])\n",
    "                f.write(str(topictable['문서 번호'][i])+\"\\n\") #불러올 때 string = f.read()후, string.split(\"\\n\")할 것\n",
    "#print(filter_doc)\n",
    "print(len(filter_doc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
